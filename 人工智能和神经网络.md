# 90分钟，一口气搞懂人工智能和神经网络

> B站up主：**漫士沉思录**
> 【90分钟！清华博士带你一口气搞懂人工智能和神经网络】 https://www.bilibili.com/video/BV1atCRYsE7x/?share_source=copy_web&vd_source=769ad23db0f196e2d0701d2edd5e74a2

智能本质上就是针对不同情景给出针对性的输出反应

图灵测试

智能本质上是一个黑箱，可以找到输入和输出之间的对应关系

符号主义 vs. 机器学习（联结主义）

机器学习

- 哪来这种神奇黑箱？—— 模型结构
- 怎么奖励一个机器？—— 损失函数
- 机器怎么建立条件反射？—— 训练过程

联结主义Connectionism

感知机：模式识别算法模型

![59fe3a801d2dc641b9a1357731197e11](人工智能和神经网络.assets/59fe3a801d2dc641b9a1357731197e11.jpeg)

![d539768eea9a5404cfeb94ac8cc8f6f4](人工智能和神经网络.assets/d539768eea9a5404cfeb94ac8cc8f6f4.jpeg)

![cadcb0f0f0fd6d1040a2977c17e0ceeb](人工智能和神经网络.assets/cadcb0f0f0fd6d1040a2977c17e0ceeb.jpeg)

为了解决异或的问题，出现了多层感知机（MLP，Multilayer Preceptron）

![f91599c906ff7772871566f6090eabd2](人工智能和神经网络.assets/f91599c906ff7772871566f6090eabd2.jpeg)

![2f0372b7c81a49e38ef2a10662d5f96e](人工智能和神经网络.assets/2f0372b7c81a49e38ef2a10662d5f96e.jpeg)

动物的视觉，神经系统的神经元不需要和前一层的所有神经元全都稠密的连接，而只需要和局部的几个神经元连接。减少参数和运算量，提升神经网络的性能。卷积神经网络（CNN，Convolutional Neural Network）。

![c5faf2c3c77e1132cf5b1eef45ac682d](人工智能和神经网络.assets/c5faf2c3c77e1132cf5b1eef45ac682d.jpeg)

神经网络结构设计，大部分模型都在使用同一种算法来训练网络找到最好的参数——梯度下降。

拟合函数和损失函数不同

对每一个数据点，函数的预测和实际结果的偏差的平方加在一起，就得到了损失函数
损失函数就是在衡量一个模型预测的和真实的结果之间的偏差程度

梯度下降精神：每次减小一点点

计算梯度——反向传播（BP，back propagation）



泛化 Generalization：推广、举一反三、活学活用

对抗样本：模糊混淆人工智能对与数据识别的准确性



AGI（通用人工智能）

ChatGPT原理：预测下一个语素（next-token-prediction）

统计语言模型（Statistical Language Model）

自回归生成：把前面自己的输出当作是新的输入的条件的方式

让大语言模型写一句刚好20个字的话，是无法做到的

语言模型用有 Hallucination（幻觉）

大语言模型训练过程：

- 监督训练微调 Supervised Fintune
- 人类反馈强化 RLHF，Reinforcement Learning with Human Feedback



扩散模型（Diffusion）原理（OpenAI 的 Sora）

2020年论文，创造性地利用了扩散过程，将它变成了一个可以用来生成图片的算法，这就是扩散生成模型——Denoising Diffusion Probabilistic Models（https://arxiv.org/abs/2006.11239）

Sora在长程依赖（long range dependency）上做的很好，长程依赖即每个物品不会凭空出现或消失

